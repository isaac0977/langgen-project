# -*- coding: utf-8 -*-
"""LangGen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1owUFMylSfCk1UdiZXweQvm-2LpyhU_d9
"""

from google.colab import drive
drive.mount('/content/drive')

#The latest version of the community detection has a bug (enters an infinite loop if all the setences belong to the same cluster, so etting this library from a specific git commit)
#!pip install git+https://github.com/UKPLab/sentence-transformers.git@d8982c9f0d44f8a3c41579fa64c603eca029649b

#Helper function that combines two prices if they are for similar items
def take_price_average(price_range):
  if '-' in price_range:
    min_max = price_range.split('-')
    min = min_max[0].strip()
    max = min_max[1].strip()
    try:
      min_price = float(min.split('$')[1].replace(',',''))
      max_price = float(max.split('$')[1].replace(',',''))
    except:
      return 

    return (min_price + max_price) / 2
  else:
    output = ""
    try:
      output = float(price_range.split('$')[1].replace(',',''))
      return output
    except:
      print(price_range)
    #return float(price_range.split('$')[1].replace(',',''))

def take_rank_average(rank1, rank2)

#Text tokenizer functions

import re
from typing import Callable, List
def create_integer_extractor() -> Callable[[str], str]:
  def normalize(text: str) -> str:
    if text == "" or text.isspace():
      return None
    else:
      extracted = re.search(r'\b\d[\d,.]*\b', text)
      ret_val = int(extracted.group().replace(',', ''))
      return ret_val
  
  return normalize
  
def create_text_normalizer(strip_punctuation_and_lowercase: bool = True) -> Callable[[str], str]:
    remove_patterns = list(map(
        re.compile,
        [
            r'<<[^>]*>>',  # <<>>
            r'<[^>]*>',  # <>
            r'\(\([^)]*\)\)',  # (( ))
            r'\([^)]*\)',  # ... ?
            r'\(',  # unbalanced parentheses
            r'\)',  #
            r'#',  # comments?
            r'\*.+',  # comments about typos: e.g. "i think their *their -> they're"
            r'[0-9]{4,}', #numbers with too many digits
            r'[0-9]*.*quot', #removes 9" that became '9quot'
        ]
    ))

    if strip_punctuation_and_lowercase:
        remove_patterns.append(re.compile(r'[!"#$%&()*+,./:;=?@\[\\\]^_`{|}~]'))

    remove_leading_nontext = re.compile(r'^[^a-zA-Z<]+([a-zA-Z])')
    correct_punctuation_whitespace = re.compile(r' ([.,?!])')
    wild_dashes = re.compile(r'(\s-+\s|-+$)')

    def normalize(text: str) -> str:
        for p in remove_patterns:
            text = p.sub('', text)
        text = text.replace('--', '')
        text = text.split('*')[0].strip()  # Comments after asterisk
        text = remove_leading_nontext.sub(r'\1', text)  # ". . Hi again." => "Hi again."
        text = correct_punctuation_whitespace.sub(r'\1', text)  # "Hi Jack ." -> "Hi Jack."
        if strip_punctuation_and_lowercase:
            text = wild_dashes.sub(' ', text).strip()
            text = text.lower()
        return ' '.join(text.split())

    return normalize

def create_text_normalizer_list(strip_punctuation_and_lowercase: bool = True) -> Callable[[str], List[str]]:
    remove_patterns = list(map(
        re.compile,
        [
            r'<<[^>]*>>',  # <<>>
            r'<[^>]*>',  # <>
            r'\(\([^)]*\)\)',  # (( ))
            r'\([^)]*\)',  # ... ?
            r'\(',  # unbalanced parentheses
            r'\)',  #
            r'#',  # comments?
            r'\*.+',  # comments about typos: e.g. "i think their *their -> they're"
             r'[0-9]{4,}', #numbers with too many digits
            r'[0-9]*.*quot', #removes 9" that became '9quot'
        ]
    ))

    if strip_punctuation_and_lowercase:
        remove_patterns.append(re.compile(r'[!"#$%&()*+,./:;=?@\[\\\]^_`{|}~]'))

    remove_leading_nontext = re.compile(r'^[^a-zA-Z<]+([a-zA-Z])')
    correct_punctuation_whitespace = re.compile(r' ([.,?!])')
    wild_dashes = re.compile(r'(\s-+\s|-+$)')

    def normalize(texts: List[str]) -> List[str]:
        return_list = []
        for text in texts:
          for p in remove_patterns:
              text = p.sub(' ', text)
          text = text.replace('--', '')
          text = text.split('*')[0].strip()  # Comments after asterisk
          text = remove_leading_nontext.sub(r'\1', text)  # ". . word1 word2." => "word1 word2."
          text = correct_punctuation_whitespace.sub(r'\1', text)  # "word1 word2 ." -> "word1 word2."
          if strip_punctuation_and_lowercase:
              text = wild_dashes.sub(' ', text).strip()
              text = text.lower()
          if len(text) > 2:
            if text[0] == "\[":
              text = text[1:]
            if text[-1] == "]":
              text = text[:-1]
          return_list.append(' '.join(text.split()))
        #return return_list
        return ' '.join(return_list)

    return normalize

#Create sentence embeddings, used for community detection

from sentence_transformers import SentenceTransformer, util
import torch

def create_sentence_embeddings(col_name, df):
  title_list = df[col_name].values.tolist()
  #title_list= title_list[:5000]
  sentences = title_list
  model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
  sentence_embeddings = model.encode(sentences)
  return sentences, sentence_embeddings

def cluster_sentences(sentence_embeddings):
  clusters = util.community_detection(embeddings=torch.tensor(sentence_embeddings), min_community_size=2, threshold=0.80)
  return clusters

#If multiple rows are for similar items, then collapse these into one line
import numpy as np


def collapse_similar_items(df, sentences, clusters):
  for i, cluster in enumerate(clusters):
    unique_titles = set()
    for sentence_id in cluster:
        unique_titles.add(sentences[sentence_id])
    rows = []
    sub_df = df[df["title"].isin(unique_titles)]
    if(sub_df.shape[0] > 0) :
        df = df.drop(sub_df.index)
        average_price = np.mean(sub_df.price)
        sub_df.iloc[0].price = average_price
        sub_df = sub_df.iloc[[0]]
        df.loc[len(df.index)] = sub_df.iloc[0]
  return df

import pandas as pd
import os

def parse_and_clean_dataset_chunk(df):

  #Open the file, remove rows with values that are too long or missing
  df = df[df['price'].astype(bool)]
  #df['feature'] = df['feature'].replace([], np.nan, inplace=True)
  df = df.dropna(subset=['title', 'brand', 'feature', 'description', 'rank'])
  df = df[df['price'].str.len() <= 500]
  df = df[df['title'].str.len() <= 500]
  df = df[df['description'].str.len() > 1]
  #Remove HTML like strings
  df = df[~df.price.str.contains('<div')]
  df = df[~df.title.str.contains('<div')]
  df[df['description'].str.strip().astype(bool)]
  df = df.dropna(subset=['title', 'brand', 'feature', 'description', 'rank'])
  df["rank"] = df["rank"].apply(''.join)
  #Convert prices to float
  df['price'] = df['price'].apply(take_price_average)

  #Text normalization
  normalize_text = create_text_normalizer(strip_punctuation_and_lowercase=True)
  normalize_text_list = create_text_normalizer_list(strip_punctuation_and_lowercase=True)
  extract_integers = create_integer_extractor()
  df.title = df.title.apply(normalize_text)
  df.feature = df.feature.apply(normalize_text_list)
  df.description = df.description.apply(normalize_text_list)
  df["rank"] = df["rank"].apply(extract_integers)
  df = df[df['title'].str.len() > 10]
  df = df.dropna(subset=['title', 'brand', 'feature', 'description', 'rank'])
  print(df.shape)

  if (df.shape[0] > 2):
    #Removing duplicate rows
    sentences, embeddings = create_sentence_embeddings("title", df)
    clusters = cluster_sentences(embeddings)
    
    total_sentences = 0
    for cluster in clusters:
      for sentence in cluster:
        total_sentences += 1
    print("number of clustered sentences:")
    print(total_sentences)
    df = collapse_similar_items(df, sentences, clusters)
    print("number of clusters:")
    print(len(clusters))
    print("Final size")
    print(df.shape)
    print()

  return df[['description', 'title', 'feature', 'rank', 'price']]

import pandas as pd


#Main Execution
for file_name in os.listdir('/content/drive/MyDrive/raw_amazon_data/'):
  if os.path.exists(os.path.join('/content/drive/MyDrive/amazon_data/',file_name[:-8]+'.csv')):
    continue
  chunks = pd.read_json(os.path.join('/content/drive/MyDrive/raw_amazon_data/',file_name), lines=True, chunksize=1000, compression='gzip')
  for chunk in chunks:
    chunk = parse_and_clean_dataset_chunk(chunk)
    chunk.to_csv(os.path.join('/content/drive/MyDrive/amazon_data/',file_name[:-8]+'.csv'), mode='a', index=False, header=False)
