# -*- coding: utf-8 -*-
"""LangGen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1owUFMylSfCk1UdiZXweQvm-2LpyhU_d9
"""


import math
import pandas as pd
import os

desirable = pd.DataFrame(columns=["description", "title", "feature"])
undesirable = pd.DataFrame(columns=["description", "title", "feature"])

for file_name in os.listdir('/content/drive/MyDrive/amazon_data_clusters/'):
  df = pd.read_csv(os.path.join('/content/drive/MyDrive/amazon_data_clusters/',file_name))
  df = df.dropna()
  print()
  print(file_name)
  print(df.shape[0])

  desc = 0
  feat = 0
  title = 0
  for i in range(df.shape[0]):
    description = df.iloc[i]['description']
    desc += len(description.split(' '))
    feature = df.iloc[i]['feature']
    feat += len(feature.split(' '))
    ti = df.iloc[i]['title']
    title += len(ti.split(' '))

  print ('desc')
  if (df.shape[0] != 0):
    print(desc/df.shape[0])
    print(feat/df.shape[0])
    print(title/df.shape[0])

    skewedness = 0

    #For each cluster, get the zscore of the prices, and apply math.exp(-x).
    #Multiply the rank of this item by the modified price
    #Sort each cluster by the modified rank
    #The top 15 percent is the desirable set, the bottom 15 percent is the cheap set
    for cluster in df["cluster"].unique():
      sub_df = df[df['cluster']==cluster]
      sub_df["priceZ"] = (sub_df["price"]-sub_df["price"].mean())/sub_df["price"].std()
      skewedness += sub_df.skew(axis='columns', numeric_only=True).iloc[1]

      sub_df["modifiedPrice"] = sub_df['priceZ'].apply(lambda x: math.exp(-x))
      sub_df["modifiedRank"] = sub_df["modifiedPrice"]*sub_df["rank"]
      sub_df=sub_df.sort_values("modifiedRank")

      cutoff = int(sub_df.shape[0]*0.15)
      highs = sub_df.iloc[:cutoff][['description', 'title', 'feature']]
      desirable = pd.concat([desirable,highs])
      lows = sub_df.iloc[-cutoff:][['description', 'title', 'feature']]
      undesirable = pd.concat([undesirable,lows])

test = desirable.dropna()
test.shape




import nltk
nltk.download('averaged_perceptron_tagger')

#Find the most popular adjectives in both cheap and expensive sets
def lexicon_induction(df):
  lexicon = {}
  for i in range(df.shape[0]):
    description = df.iloc[i]['description']
    tags = nltk.pos_tag(description.split(' '))
    for tag in tags:
      
      if (tag[1][:2] == 'JJ'):
        if tag[0] not in lexicon:
          lexicon[tag[0]] = 1
        else:
          lexicon[tag[0]] += 1
  return lexicon

#Remove the popular words
def clean_up_words(df, word_set):
  for i in range(df.shape[0]):
    description = df.iloc[i]['description']
    split_words = description.split(' ')
    cleaned = []
    for word in split_words:
      if word not in word_set:
        cleaned.append[word]
    df.iloc[i]['description'] = ' '.join(cleaned)
  return df

desirable_lexicon = lexicon_induction(desirable)
undesirable_lexicon = lexicon_induction(undesirable)

import operator
desirable_set = set()
undesirable_set = set()
for k,v in sorted(desirable_lexicon.items(), key=operator.itemgetter(1))[-700:]:
    desirable_set.add(k)
for k,v in sorted(undesirable_lexicon.items(), key=operator.itemgetter(1))[-700:]:
    undesirable_set.add(k)

desirable_words = desirable_set - undesirable_set
undesirable_words = undesirable_set - desirable_set
print(desirable_words)
print(undesirable_words)

desirable = clean_up_words(desirable, desirable_words+undesirable_words)
undesirable = clean_up_words(undesirable, desirable_words+undesirable_words)

desirable.dropna().to_csv(os.path.join('/content/drive/MyDrive/amazon_data/expensive.csv'), index=False, header=True)

undesirable.dropna().to_csv(os.path.join('/content/drive/MyDrive/amazon_data/cheap.csv'), index=False, header=True)
